{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459d0baf",
   "metadata": {},
   "source": [
    "# Supervised Learning - Classification Problem\n",
    "\n",
    "## Students' dropout and academic success\n",
    "\n",
    "### (Droupout, Enrolled, Graduate)\n",
    "\n",
    "Faculdade: FEUP - Faculdade de Engenharia da Universidade do Porto\n",
    "\n",
    "Curso: L.EIC - Licenciatura em Engenharia Informática e Computação \n",
    "\n",
    "Unidade Curricular: Inteligência Artificial\n",
    "\n",
    "Ano Curricular: 2021/22\n",
    "\n",
    "Grupo: 21_1D\n",
    "\n",
    "Elementos:\n",
    "- Henrique Ribeiro Nunes, up201906852@up.pt\n",
    "- Margarida Assis Ferreira, up201905046@up.pt\n",
    "- Patrícia do Carmo Nunes Oliveira, up201905427@up.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282d84f",
   "metadata": {},
   "source": [
    "### Especificação\n",
    "\n",
    "O foco principal deste problema é analisar a informação conhecida sobre a matrícula do aluno (percurso acadêmico, demografia e fatores socioeconómicos) e o desempenho académico dos alunos no final do primeiro e segundo semestres. Com o objetivo de usar estes dados para construir modelos de classificação para **prever a desistência e o sucesso académico dos alunos**.\n",
    "\n",
    "Este problema é um ***single label multiclass classification problem*** com 37 atributos:\n",
    "- 36 métricas distintas para descrever as informações do aluno.\n",
    "- 1 objetivo com 3 resultados possíveis (*Droupout*, *Enrolled*, *Graduate*).\n",
    "\n",
    "Existe um **forte desbalanceamento** em relação a um dos resultados possíveis.\n",
    "\n",
    "\n",
    "### Tools & Resources\n",
    "\n",
    "// TODO AQUI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9d948",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "A análise dos dados é um passo importante nos problemas de classificação. \n",
    "\n",
    "Nesta secção são analisados os atributos que classificam os dados e o tipo e intervalo de valores de cada um dos atributos, bem como o tamanho do conjunto de dados, a presença de valores nulos ou amostras duplicadas. Igualmente é explorado a distribuição das classes e de valores por atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35be874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d65b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print head values and summary statistics\n",
    "print(data.describe())\n",
    "print()\n",
    "print(data.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all 36 metrics used and 3 possible results\n",
    "metrics = list(data.columns)\n",
    "metrics.remove(\"class\")\n",
    "print(\"Number of Metrics: {}\".format(len(metrics)))\n",
    "print(\"Metrics: {}\".format(metrics))\n",
    "values = list(data[metrics].values)\n",
    "\n",
    "classes = list(data[\"class\"].unique())\n",
    "print(\"\\nNumber of Results: {}\".format(len(classes)))\n",
    "print(\"Result/Prediction: {}\".format(classes))\n",
    "targets = list(data[\"class\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5121250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check atribute types and values interval\n",
    "for name, dtype in data.dtypes.iteritems():\n",
    "    print(\"{} | {} | [{} , {}] \".format(name.ljust(46), str(dtype).ljust(7), data[name].min(), data[name].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "print(\"Data Size: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are columns with N/A values\n",
    "print(\"N/A values found: {}\".format(data.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicated Data\n",
    "bool_series = data.duplicated()\n",
    "print(bool_series)\n",
    "\n",
    "old_size = len(data)\n",
    "\n",
    "# Removing all duplicated data if exists\n",
    "data = data[~bool_series] \n",
    "\n",
    "new_size = len(data)\n",
    "\n",
    "# check if there were actualy duplicated data\n",
    "print()\n",
    "if (new_size == old_size):\n",
    "    print(\"No data was removed: there were no duplicated data\")\n",
    "else:\n",
    "    print(\"Was found and removed {} duplicated data\".format(old_size-new_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02cd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resultant data without duplicated entries\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72983775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for each feture values and respective count of classses\n",
    "\n",
    "def count_targets_for_each_value(data, feat):\n",
    "    data_aux = data[[feat, \"class\"]]\n",
    "    d = {}\n",
    "    for line in data_aux.values:\n",
    "        if not line[0] in d:\n",
    "            d[line[0]] = {classes[0]: 0, classes[1]: 0, classes[2]: 0}\n",
    "        d[line[0]][line[1]] += 1\n",
    "\n",
    "    for entry in d.keys():\n",
    "        d[entry] = [d[entry][targets[c]] for c in range(3)]\n",
    "    aux = pd.DataFrame(d, index=classes)\n",
    "    return aux.reindex(sorted(aux.columns), axis=1)\n",
    "\n",
    "\n",
    "for i in range(36):\n",
    "    plt.title(metrics[i])\n",
    "    plt.hist(data[metrics[i]].values)\n",
    "    plt.show()\n",
    "    print(count_targets_for_each_value(data, metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution\n",
    "# Check data balance\n",
    "n_graduate = data['class'].value_counts().Graduate\n",
    "n_dropouts = data['class'].value_counts().Dropout\n",
    "n_enrroled = data['class'].value_counts().Enrolled\n",
    "print(\"Number of 'Graduate' occurences: {}\".format(n_graduate))\n",
    "print(\"Number of 'Dropout' occurences: {}\".format(n_dropouts))\n",
    "print(\"Number of 'Enrolled' occurences: {}\".format(n_enrroled))\n",
    "print()\n",
    "\n",
    "# Corresponding plot \n",
    "count_result = pd.DataFrame(data[\"class\"]).value_counts().rename_axis(\"class\").reset_index(name=\"count\")\n",
    "print(count_result)\n",
    "\n",
    "# Bar plot\n",
    "plot_count_res = sb.barplot(data=count_result, x=\"count\", y=\"class\")\n",
    "plt.show()\n",
    "\n",
    "# Pie plot with percentages\n",
    "plt.pie([n_graduate, n_dropouts, n_enrroled], autopct = '%0.00f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af818892",
   "metadata": {},
   "source": [
    "Analisando a distribuição de cada atributo pelas classes possíveis, podemos concluir que existem alguns atributos irrelevantes, na medida que não se distingue à priori nem se obtem qualquer informação sobre qual a classe mais provável para uma nova amostra com base no valor desse atributo, como conseguimos identifificar no caso do atributo 'Curricular units 2nd sem (credited)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of each attribute \n",
    "def densityPlot(x, hue, fill):\n",
    "    sb.displot(data, x=x, hue=hue, kind=\"kde\", fill=fill)\n",
    "    \n",
    "# all 36 distinct attributes\n",
    "attributes = list(data.columns)\n",
    "attributes.remove(\"class\")\n",
    "    \n",
    "for attribute in attributes:\n",
    "    densityPlot(attribute, 'class', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0b482",
   "metadata": {},
   "source": [
    "**Propriedades do Problema:** (a partir da análise dos dados de entrada)\n",
    "\n",
    "- Nominal and Discrete attributes (including some binary ones)\n",
    "- Dimensionality = 37 attibutos\n",
    "- Size = 4424\n",
    "- Type = Data Matrix\n",
    "- No missing or duplicate Data\n",
    "- No meaningful outliers\n",
    "- Imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078f052",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados\n",
    "\n",
    "Tendo em conta as conclusões obtidas pela a análise dos dados originais mostrada anteriormente, podemos apercebermo-nos que é necessário realizar um pré-processamento dos dados, com o objetivo de resolver o maior problema adjacente a estes: a falta de balanceamento entre as diferentes classes de alvo. \n",
    "\n",
    "Para tal podemos usar entre outras estratégias uma das seguintes:\n",
    "- **oversampling** : «Aumentar o número de amostras/entradas da menor classe até coincidir com o tamanho da maior classe»\n",
    "- **undersampling** : «Diminuir o número de amostras/entradas da maior classe até coincidir com o tamanho da menor classe»\n",
    "\n",
    "Em qualquer uma das abordagens acima, a escolha das amostras escolhidas para serem retidas ou replicadas com pequenas modificações é aleatória."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747535b",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "\n",
    "Uma das técnicas para lidar com o desbalanceamento de classes em machine lerning é chamado de *undersampling*. Esta técnica de balanceamento consiste em remover algumas observações das classes majoritárias, até que as classes majoritárias e minoritárias sejam equilibradas. A técnica *undersampling* pode ser uma boa escolha quando temos dados desequilibrados, mas uma desvantagem é que removemos informações que podem ser valiosas.\n",
    "\n",
    "Para remover as observações das classes majoritárias, usamos a função `sample(sequence, k)`, uma função do módulo `Random` de Python, que retorna uma lista de comprimento `k` de itens escolhidos aleatoriamente de `sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance Original Data\n",
    "print(\"Classes count:\")\n",
    "print(data['class'].value_counts())\n",
    "\n",
    "data['class'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_graduate = data[data['class'] == \"Graduate\"]\n",
    "class_dropout = data[data['class'] == \"Dropout\"]\n",
    "class_enrolled = data[data['class'] == \"Enrolled\"]\n",
    "\n",
    "class_dropout_under = class_dropout.sample(n_enrroled, replace=True)\n",
    "class_graduate_under = class_graduate.sample(n_enrroled, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de34028",
   "metadata": {},
   "source": [
    "Agora temos os nossos dados balanceados, como é possível observer no gráfico criado pelo código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_under = pd.concat([class_dropout_under, class_graduate_under, class_enrolled], axis=0)\n",
    "\n",
    "# plot the count after under-sampeling\n",
    "print(\"Classes count after under-sampeling:\")\n",
    "print(test_under['class'].value_counts())\n",
    "\n",
    "test_under['class'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545b9f9",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "\n",
    "// TODO AQUI -> improve description\n",
    "Segue o seguinte esquema: [incluir imagem significativa]\n",
    "\n",
    "As entradas da classe menor sáo replicadas até totalizarem o número de amostras da classe maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795eb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance Original Data\n",
    "unbalanced_count = data['class'].value_counts()\n",
    "unbalanced_count.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate count and samples by the diferent classes\n",
    "graduate_count, droupout_count, enrolled_count = data['class'].value_counts()\n",
    "\n",
    "print(\"GRADUATE: {} | DROUPOUT: {} | ENROLLED: {}\".format(graduate_count, droupout_count, enrolled_count))\n",
    "\n",
    "graduate_samples = data[data['class'] == \"Graduate\"]\n",
    "droupout_samples = data[data['class'] == \"Dropout\"]\n",
    "enrolled_samples = data[data['class'] == \"Enrolled\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69583837",
   "metadata": {},
   "source": [
    "#### Random Over-Sampling\n",
    "\n",
    "«Oversampling can be defined as adding more copies to the minority class.»\n",
    "\n",
    "**Disvantagens:** pod causar *overfitting* e pobre generalização do conjunto de dados para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of the smallest to the larger ones\n",
    "droupout_samples_over = droupout_samples.sample(graduate_count, replace=True)\n",
    "enrolled_samples_over = enrolled_samples.sample(graduate_count, replace=True)\n",
    "\n",
    "test_samples_over = pd.concat([graduate_samples, droupout_samples_over, enrolled_samples_over], axis=0)\n",
    "\n",
    "print(\"Total dintinct classes: \\n{}\".format(test_samples_over['class'].value_counts()))\n",
    "\n",
    "rnd_oversampling_count = test_samples_over['class'].value_counts()\n",
    "rnd_oversampling_count.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8f808",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "// TODO AQUI - NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baeb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "# TODO find want is supposted to be x and y \n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_ros))\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "# fit predictor and target variable\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb43ab",
   "metadata": {},
   "source": [
    "É importante referir que nenhuma das soluções acima é uma solução perfeita, pois a aplicação de undersampling pode inplicar a perda de infromação, da mesma forma que a aplicação de oversampling (sem qualquer modificação das amostras escolhidas aleatóriamente para serem replicadas) pode levar a um posterior overfitting dos modelos gerados a estes novos dados.\n",
    "\n",
    "\n",
    "#### Combine under and over sampling\n",
    "\n",
    "Tendo isto em conta a seguinte tentativa tenta encontrar um meio termos entre as soluções anteriores, fazendo as classes em questão convergir para um valor mediano e não para um máximo nem minímo, tentando combater as consequencias sentidas nos dados ao aplicar isoladamente cada uma das estratégias, obtando por alcaçar um meio termo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the counts meet at the middle point\n",
    "# in this case the middle point is consider to be the dropout class\n",
    "graduate_samples_middle = graduate_samples.sample(droupout_count, replace=True)\n",
    "enrolled_samples_middle = enrolled_samples.sample(droupout_count, replace=True)\n",
    "\n",
    "test_samples_middle = pd.concat([graduate_samples_middle, droupout_samples, enrolled_samples_middle], axis=0)\n",
    "\n",
    "print(\"Total dintinct classes: \\n{}\".format(test_samples_middle['class'].value_counts()))\n",
    "\n",
    "middle_sampling_count = test_samples_middle['class'].value_counts()\n",
    "middle_sampling_count.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62685cc7",
   "metadata": {},
   "source": [
    "## Learning Algorithms\n",
    "\n",
    "### Identification of the Target Concept\n",
    "\n",
    "// TODO AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38110f8",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e072307",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, classes, test_size):\n",
    "  feat_train, feat_test, target_train, target_test = train_test_split(data, classes, test_size=test_size, shuffle=False)\n",
    "  return feat_train, feat_test, target_train, target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979274f",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4df76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def cross_validation(model, features, targets, cv):\n",
    "    scores = cross_val_score(model, features, targets, cv=cv)\n",
    "    print(\"Cross validation: {:.2f} accuracy with a standard deviation of {:.2f}\".format(scores.mean(), scores.std()))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcece04",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e19664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def confusion_matrix(taget_test, target_predictions):    \n",
    "    return metrics.confusion_matrix(target_test, target_predictions)\n",
    "\n",
    "def display_confusion_matrix(target_test, target_predictions):\n",
    "    metrics.ConfusionMatrixDisplay.from_predictions(target_test, target_predictions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc2d50",
   "metadata": {},
   "source": [
    "#### Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def validate(model, features, targets):\n",
    "    test_size = 0.2\n",
    "    cross_validation_split = 5\n",
    "    feat_train, feat_test, target_train, target_test = split_data(features, targets, test_size=test_size)\n",
    "    \n",
    "    model.fit(feat_train, target_train)\n",
    "    predictions = model.predict(feat_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(target_test, predictions)\n",
    "    print(\"Accuracy: {:.2f}\\n\".format(acc*100))\n",
    "    \n",
    "    display_confusion_matrix(target_test, predictions)\n",
    "    \n",
    "    cross_validation(model, features, targets, cross_validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae85bd3",
   "metadata": {},
   "source": [
    "#### Selection and Parameterization of the learning algorithms to employ\n",
    "\n",
    ":: At least 3 supervised learning (classification) algorithms should be employed (Decision Trees, Neural Networks, KNN, SVM, …) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f15259",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99882e27",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea410dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier\n",
    "dtc = DecisionTreeClassifier(\n",
    "  criterion=\"gini\", # criterion{“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "  splitter=\"best\", # splitter{“best”, “random”}, default=”best”\n",
    "  max_depth=5, \n",
    "  max_features=None, # max_featuresint, float or {“auto”, “sqrt”, “log2”}, default=None\n",
    "  max_leaf_nodes=None) # max_leaf_nodes: int, default=None\n",
    "validate(dtc, values, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF with the tree\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(dtc, out_file=None, feature_names=labels, class_names=targets ) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"decision_tree\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15935928",
   "metadata": {},
   "source": [
    "#### K-nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e51384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbors = 6\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors,weights=\"uniform\")\n",
    "validate(knc, values, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "'''\n",
    "_, ax = plt.subplots()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X=np.array([line[:2] for line in data_train]),\n",
    "    ax=ax,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    shading=\"auto\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "        x=data[labels[0]],\n",
    "        y=data[labels[1]],\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95b4b0",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2084bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nnc = MLPClassifier(\n",
    "    solver='lbfgs', \n",
    "    alpha=1e-5,\n",
    "    hidden_layer_sizes=(5, 2),\n",
    "    max_iter=200,\n",
    "    random_state=1)\n",
    "\n",
    "validate(nnc, values, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8268ff",
   "metadata": {},
   "source": [
    "#### Evaluation of the learning process\n",
    "\n",
    ":: compared using appropriate evaluation\n",
    "metrics (performance during learning, confusion matrix, precision, recall, accuracy, F1 measure) and the time\n",
    "spent to train/test the models.\n",
    "(in particular on the test set)\n",
    "\n",
    ":: compared using the Scikit-Learn Python library and considering\n",
    "the characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f8372",
   "metadata": {},
   "source": [
    "#### Results Comparision\n",
    "\n",
    ":: Results should be compared using tables or plots (e.g., using Seaborn or\n",
    "Matplotlib libraries)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
